{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from constants import REPO_DIR\n",
    "from utils import get_saved_model, NormalizeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.normal = torch.distributions.Normal(0, 1)\n",
    "    self.pre_latent_dim = 128\n",
    "    self.latent_dim = 16\n",
    "    self.latent_to_mu = nn.Linear(self.pre_latent_dim, self.latent_dim)\n",
    "    self.latent_to_sigma = nn.Linear(self.pre_latent_dim, self.latent_dim)\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "        nn.ReLU(True),\n",
    "        nn.Flatten(start_dim=1),\n",
    "        nn.Linear(3 * 3 * 32, self.pre_latent_dim),\n",
    "        nn.ReLU(True),\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(self.latent_dim, self.pre_latent_dim),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(self.pre_latent_dim, 3 * 3 * 32),\n",
    "        nn.ReLU(True),\n",
    "        nn.Unflatten(dim=1, \n",
    "        unflattened_size=(32, 3, 3)),\n",
    "        nn.ConvTranspose2d(32, 16, 3, \n",
    "        stride=2, output_padding=0),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
    "        padding=1, output_padding=1),\n",
    "        nn.BatchNorm2d(8),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
    "        padding=1, output_padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    mu = self.latent_to_mu(x)\n",
    "    sigma = self.latent_to_sigma(x)\n",
    "    z = mu + sigma * self.normal.sample(self.latent_dim)\n",
    "    x = self.decoder(z)\n",
    "    return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model():\n",
    "    \"\"\"\n",
    "    @returns: retrains new model\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToTensor(), NormalizeImage()])\n",
    "    training_set = tv.datasets.MNIST(root = './data', train = True, download = True, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(training_set, batch_size=32, shuffle=False)\n",
    "\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "    model = AutoEncoder().cpu()\n",
    "    distance = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    "            #note, img from dataloader has dimensions 32x28x28 (batch size is 32)\n",
    "            #Conv2d input shape is 32x1x28x28 (batch_size x num_channels x width x height)\n",
    "            img, _ = data\n",
    "            img = img.reshape(batch_size, -1, 28, 28)\n",
    "            img = torch.autograd.Variable(img).cpu()\n",
    "            # ===================forward=====================\n",
    "            output = model(img)\n",
    "            loss = distance(output, img)\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    return model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_DEFAULT_WEIGHT_PATH = os.path.join(REPO_DIR, 'architectures', 'weights', 'vae_state_dict.pt')\n",
    "\n",
    "def save_model(model, weight_path=VAE_DEFAULT_WEIGHT_PATH):\n",
    "    \"\"\"\n",
    "    @param model: model to save\n",
    "    @param weight_path: path to save weights to\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(weight_path):\n",
    "        os.makedirs(os.path.dirname(weight_path), exist_ok = True)\n",
    "    torch.save(model.state_dict(), weight_path)\n",
    "\n",
    "\n",
    "def load_saved_model(weight_path=VAE_DEFAULT_WEIGHT_PATH):\n",
    "    \"\"\"\n",
    "    @param weight_path: points to path where weights are stored to load model\n",
    "    @returns: Trained model\n",
    "    @throws: Exception if no weights stored at weight_path\n",
    "    \"\"\"\n",
    "    return get_saved_model(VariationalAutoEncoder, weight_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa50d98202e836e57a8cfa0defb25a13d588a147734f328b89b2b92a7b90c31b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
